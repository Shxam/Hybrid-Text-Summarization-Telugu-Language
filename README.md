# Hybrid Text Summarization for Telugu Language ğŸ“ğŸ‡®ğŸ‡³

This project implements a **hybrid text summarization** approach for the **Telugu language**, combining both **extractive** and **abstractive** methods. It uses **TextRank** for extractive summarization and a fine-tuned **mT5-small transformer model** for abstractive summarization.

---

## ğŸ“‚ Repository Contents

- `EXTRACTIVE_SUMMARIZATION.ipynb`: Implementation using **TextRank** algorithm.
- `ABSTRACTIVE_TRANSFORMER.ipynb`: Fine-tuning **mT5-small** for abstractive summarization.
- `SEQ2SEQ_pytorch.ipynb` and `seq2seq_tensorflow.ipynb`: Experimental comparison with other seq2seq models.
- `model_compare.ipynb`: ROUGE score evaluation and model performance.
- `mini_t5model/`: Contains saved fine-tuned mT5 model files.
- `telugu_XLSum_v2.0.tar.bz2`: Dataset used for training and evaluation.
- `A12_report.pdf / A12_report.zip`: Final project documentation.
- `LICENSE`: MIT License.

---

## ğŸš€ Getting Started

### ğŸ” Clone the Repository

```bash
git clone https://github.com/Shxam/Hybrid-Text-Summarization-Telugu-Language.git
cd Hybrid-Text-Summarization-Telugu-Language
ğŸ“¦ Setup Environment
bash
Copy
Edit
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
ğŸ§  Methodology
ğŸ“Œ Extractive Summarization
Algorithm: TextRank

Input: Raw Telugu document

Output: Ranked key sentences

ğŸ“Œ Abstractive Summarization
Model: google/mt5-small

Fine-tuned on extractive summaries and human-written targets

Language: Telugu

Training: 5 epochs, batch size 10, max input length 512

Tokenizer: mt5 tokenizer (supports multilingual text)

)

ğŸ“Š ROUGE Evaluation
Metric	Score
ROUGE-1	0.4321
ROUGE-2	0.3625
ROUGE-L	0.4114

ğŸ’¡ Fine-tuning with extractive summaries as input improved ROUGE scores by ~22% over baseline mT5 generation.

.

ğŸ§ª Sample Inference (Abstractive)
from transformers import MT5ForConditionalGeneration, MT5Tokenizer

tokenizer = MT5Tokenizer.from_pretrained("google/mt5-small")
model = MT5ForConditionalGeneration.from_pretrained("./mini_t5model")

text = "à°¹à±ˆà°¦à°°à°¾à°¬à°¾à°¦à± à°¸à±†à°‚à°Ÿà±à°°à°²à± à°¯à±‚à°¨à°¿à°µà°°à±à°¶à°¿à°Ÿà±€ (HCU) à°­à±‚à°®à±à°² à°µà°¿à°µà°¾à°¦à°‚ à°°à°¾à°·à±à°Ÿà±à°°à°µà±à°¯à°¾à°ªà±à°¤à°‚à°—à°¾ à°¤à±€à°µà±à°° à°šà°°à±à°šà°¨à±€à°¯à°¾à°‚à°¶à°‚à°—à°¾ à°®à°¾à°°à°¿à°‚à°¦à°¿..."
input_ids = tokenizer.encode(text, return_tensors="pt", max_length=512, truncation=True)

output = model.generate(input_ids, max_length=90, num_beams=4, early_stopping=True)
print(tokenizer.decode(output[0], skip_special_tokens=True))
Hyderabad à°¯à±‚à°¨à°¿à°µà°°à±à°¶à°¿à°Ÿà±€ à°ªà°°à°¿à°§à°¿à°²à±‹à°¨à°¿ à°¸à±à°®à°¾à°°à± 400 à°à°•à°°à°¾à°² à°­à±‚à°®à°¿à°¨à°¿ à°ªà±à°°à°­à±à°¤à±à°µ à°…à°­à°¿à°µà±ƒà°¦à±à°§à°¿ à°•à°¾à°°à±à°¯à°•à±à°°à°®à°¾à°² à°•à±‹à°¸à°‚ à°Ÿà±†à°•à±à°¨à°¾à°²à°œà±€ à°ªà°¾à°°à±à°•à± à°¨à°¿à°°à±à°®à°¾à°£à°¾à°¨à°¿à°•à°¿ à°•à±‡à°Ÿà°¾à°¯à°¿à°‚à°šà°¨à±à°¨à±à°¨à°Ÿà±à°Ÿà± à°¸à°®à°¾à°šà°¾à°°à°‚ à°µà±†à°²à±à°—à±à°²à±‹à°•à°¿ à°°à°¾à°—à°¾à°¨à±‡, à°µà°¿à°¦à±à°¯à°¾à°°à±à°¥à±à°²à±, à°ªà°°à±à°¯à°¾à°µà°°à°£ à°ªà±à°°à±‡à°®à°¿à°•à±à°²à±, à°¸à±à°¥à°¾à°¨à°¿à°•à±à°²à± à°¤à±€à°µà±à°° à°µà±à°¯à°¤à°¿à°°à±‡à°•à°¤ à°µà±à°¯à°•à±à°¤à°‚ à°šà±‡à°¸à±à°¤à±à°¨à±à°¨à°¾à°°à±.

ğŸ“‚ Project Structure
pgsql
Copy
Edit
Hybrid-Text-Summarization-Telugu-Language/
â”‚
â”œâ”€â”€ mini_t5model/                  # Fine-tuned mT5 model checkpoint
â”œâ”€â”€ A12_report.pdf / .zip          # Final report
â”œâ”€â”€ telugu_XLSum_v2.0.tar.bz2      # Dataset
â”œâ”€â”€ EXTRACTIVE_SUMMARIZATION.ipynb # TextRank implementation
â”œâ”€â”€ ABSTRACTIVE_TRANSFORMER.ipynb  # mT5 fine-tuning
â”œâ”€â”€ SEQ2SEQ_pytorch.ipynb          # Experimental PyTorch model
â”œâ”€â”€ seq2seq_tensorflow.ipynb       # TensorFlow-based baseline
â”œâ”€â”€ model_compare.ipynb            # Model evaluation and plots
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md



ğŸ™Œ Acknowledgements
Hugging Face Transformers

Indic NLP Toolkit

TextRank Algorithm

XL-Sum Telugu Dataset
